{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import subprocess\n",
    "from nltk.tokenize.moses import MosesTokenizer\n",
    "# These are both modules that I need to create files for in the folder\n",
    "from apply_bpe import BPE\n",
    "import build_data\n",
    "\n",
    "\n",
    "def create_source_target_files(input_file):\n",
    "    bpe_codes = open('/home/henrye/projects/E2E/data/trainset.2000.bpe.codes')\n",
    "    encoder = BPE(bpe_codes)\n",
    "    word_tok = MosesTokenizer(no_escape=True)\n",
    "    def tokenize(text):\n",
    "        word_tokens = word_tok.tokenize(text)\n",
    "        sub_word_tokens = encoder.segment(' '.join(word_tokens))\n",
    "        return sub_word_tokens\n",
    "    outpath = os.path.dirname(input_file)\n",
    "    file_name = os.path.basename(os.path.splitext(input_file)[0])\n",
    "    target_file = open(os.path.join(outpath, file_name + \"-target.tok.2000.bpe\"), 'w')\n",
    "    source_file = open(os.path.join(outpath, file_name + \"-source.tok.2000.bpe\"), 'w')\n",
    "    input_csv = csv.reader(open(input_file, newline=''), delimiter=',', quotechar='\"')\n",
    "    # skip the first line in the csv with the column headers\n",
    "    next(input_csv)\n",
    "#     from IPython.core.debugger import Tracer; Tracer()() \n",
    "    for line in input_csv:\n",
    "        meaning_representations = line[0].split(', ')\n",
    "        acts_tok_bpe = []\n",
    "        for act in meaning_representations:\n",
    "            act_type = act[0:act.find(\"[\")].replace(' ', '')\n",
    "            acts_tok_bpe += ['__start_' + act_type + '__']\n",
    "            acts_tok_bpe += [tokenize(act[act.find(\"[\")+1:act.find(\"]\")])]\n",
    "            acts_tok_bpe += ['__end_' + act_type + '__']\n",
    "        acts = ' '.join(acts_tok_bpe).strip()\n",
    "        target = tokenize(line[1]).strip()\n",
    "        source_file.write(acts + '\\n')\n",
    "        target_file.write(target + '\\n')\n",
    "    source_file.close()\n",
    "    target_file.close()\n",
    "            \n",
    "\n",
    "# TODO apply this to the training data\n",
    "\n",
    "def create_bpe_dict(input_file, num_operations):\n",
    "    # Inputs\n",
    "    # file name\n",
    "    # TODO BPE size\n",
    "    word_tok = MosesTokenizer(no_escape=True)\n",
    "    def tokenize(text):\n",
    "        word_tokens = word_tok.tokenize(text)\n",
    "        # I do feel like I should name the second\n",
    "        # variable something other than word_tokens\n",
    "        word_tokens = ' '.join(word_tokens)\n",
    "        return word_tokens\n",
    "    outpath = os.path.dirname(input_file)\n",
    "    file_name = os.path.basename(os.path.splitext(input_file)[0])\n",
    "    bpe_input_file = open(os.path.join(outpath, file_name + '-bpe-input.txt'), 'w')\n",
    "    input_csv = csv.reader(open(input_file, newline=''), delimiter=',', quotechar='\"')\n",
    "    # skip the first line in the csv with the column headers\n",
    "    next(input_csv)\n",
    "    for line in input_csv:\n",
    "        meaning_representations = line[0].split(', ')\n",
    "        acts_tokenized = []\n",
    "        for act in meaning_representations:\n",
    "            # https://stackoverflow.com/a/4894156/4507677 - instead of regex\n",
    "            acts_tokenized += [tokenize(act[act.find(\"[\")+1:act.find(\"]\")])]\n",
    "        acts = ' '.join(acts_tokenized)\n",
    "        target = tokenize(line[1])\n",
    "        bpe_input_file.write(acts + '\\n' + target + '\\n')\n",
    "    bpe_input_file.close()\n",
    "    # There were some errors with permissions for running this command so we're going \n",
    "    # to leave it and do it manually for now\n",
    "    # We fixed the permission issues and I might try to integrate this into the code later. We were missing\n",
    "    # execute rights on the python module\n",
    "    # ./learn_bpe.py -s {num_operations} < {train_file} > {codes_file}\n",
    "#     subprocess.run(['./learn_bpe.py', '-s', str(num_operations), '<', os.path.join(outpath, file_name + '-bpe-input.txt'),\n",
    "#                    '>', os.path.join(outpath, file_name + '.' + str(num_operations) + '.bpe.codes') ])\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "def build(dpath):\n",
    "    # dpath = ./data\n",
    "    # http://www.macs.hw.ac.uk/InteractionLab/E2E/data/traindev.zip\n",
    "    if not build_data.built(dpath):\n",
    "        print('building data')\n",
    "        if build_data.built(dpath):\n",
    "            # An older version exists, so remove the outdated files\n",
    "            # TODO it would be better if this just removed the cleaning \n",
    "            # steps rather than forcing it to redownload the data again\n",
    "            build_data.remove_dir(dpath)\n",
    "        # Download the training and validation data\n",
    "        fname = 'traindev.zip'\n",
    "        url = 'http://www.macs.hw.ac.uk/InteractionLab/E2E/data/' + fname\n",
    "        build_data.download(url, dpath, fname)\n",
    "        build_data.untar(dpath, fname)\n",
    "        \n",
    "        # TODO create BPE dict\n",
    "        create_source_target_files(os.path.join(dpath, \"trainset.csv\"))\n",
    "        # TODO Create NEMATUS dict, might not work in python 3\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[........................................] 32768 / 1394348\r",
      "[|.......................................] 65536 / 1394348\r",
      "[||......................................] 98304 / 1394348\r",
      "[|||.....................................] 131072 / 1394348\r",
      "[||||....................................] 163840 / 1394348\r",
      "[|||||...................................] 196608 / 1394348\r",
      "[||||||..................................] 229376 / 1394348\r",
      "[|||||||.................................] 262144 / 1394348\r",
      "[||||||||................................] 294912 / 1394348\r",
      "[|||||||||...............................] 327680 / 1394348\r",
      "[||||||||||..............................] 360448 / 1394348\r",
      "[|||||||||||.............................] 393216 / 1394348\r",
      "[||||||||||||............................] 425984 / 1394348\r",
      "[|||||||||||||...........................] 458752 / 1394348\r",
      "[||||||||||||||..........................] 491520 / 1394348\r",
      "[|||||||||||||||.........................] 524288 / 1394348\r",
      "[|||||||||||||||.........................] 557056 / 1394348\r",
      "[||||||||||||||||........................] 589824 / 1394348\r",
      "[|||||||||||||||||.......................] 622592 / 1394348\r",
      "[||||||||||||||||||......................] 655360 / 1394348\r",
      "[|||||||||||||||||||.....................] 688128 / 1394348\r",
      "[||||||||||||||||||||....................] 720896 / 1394348\r",
      "[|||||||||||||||||||||...................] 753664 / 1394348\r",
      "[||||||||||||||||||||||..................] 786432 / 1394348\r",
      "[|||||||||||||||||||||||.................] 819200 / 1394348\r",
      "[||||||||||||||||||||||||................] 851968 / 1394348\r",
      "[|||||||||||||||||||||||||...............] 884736 / 1394348\r",
      "[||||||||||||||||||||||||||..............] 917504 / 1394348\r",
      "[|||||||||||||||||||||||||||.............] 950272 / 1394348\r",
      "[||||||||||||||||||||||||||||............] 983040 / 1394348\r",
      "[|||||||||||||||||||||||||||||...........] 1015808 / 1394348\r",
      "[||||||||||||||||||||||||||||||..........] 1048576 / 1394348\r",
      "[|||||||||||||||||||||||||||||||.........] 1081344 / 1394348\r",
      "[|||||||||||||||||||||||||||||||.........] 1114112 / 1394348\r",
      "[||||||||||||||||||||||||||||||||........] 1146880 / 1394348\r",
      "[|||||||||||||||||||||||||||||||||.......] 1179648 / 1394348\r",
      "[||||||||||||||||||||||||||||||||||......] 1212416 / 1394348\r",
      "[|||||||||||||||||||||||||||||||||||.....] 1245184 / 1394348\r",
      "[||||||||||||||||||||||||||||||||||||....] 1277952 / 1394348\r",
      "[|||||||||||||||||||||||||||||||||||||...] 1310720 / 1394348\r",
      "[||||||||||||||||||||||||||||||||||||||..] 1343488 / 1394348\r",
      "[|||||||||||||||||||||||||||||||||||||||.] 1376256 / 1394348\r",
      "[||||||||||||||||||||||||||||||||||||||||] 1394348 / 1394348\r\n",
      "unpacking traindev.zip\n"
     ]
    }
   ],
   "source": [
    "dpath = \"./data\"\n",
    "build_data.make_dir(dpath)\n",
    "fname = 'traindev.zip'\n",
    "url = 'http://www.macs.hw.ac.uk/InteractionLab/E2E/data/' + fname\n",
    "build_data.download(url, dpath, fname)\n",
    "build_data.untar(dpath, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_file = os.path.join(dpath, \"trainset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/trainset.csv'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "this = csv.reader(open('./data/devset.csv', newline=''), delimiter=',', quotechar='\"')\n",
    "# skip the first line in the csv with the column headers\n",
    "next(this)\n",
    "for hello in this:\n",
    "    wayo = hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name[Wildwood]',\n",
       " 'eatType[coffee shop]',\n",
       " 'food[English]',\n",
       " 'priceRange[moderate]',\n",
       " 'customer rating[3 out of 5]',\n",
       " 'near[Ranch]']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wayo[0].split(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wildwood\n",
      "coffee shop\n",
      "English\n",
      "moderate\n",
      "3 out of 5\n",
      "Ranch\n"
     ]
    }
   ],
   "source": [
    "# TODO turn this into a list, join them together, tokenise it, join it back together and send to the file\n",
    "for act in wayo[0].split(\", \"):\n",
    "    print(act[act.find(\"[\")+1:act.find(\"]\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wildwood is a moderately priced English coffee shop located near the Ranch . It has been rated 3 out of 5 .'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(MosesTokenizer().tokenize(wayo[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_bpe_dict('./data/trainset.csv', 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_source_target_files('./data/devset.csv')\n",
    "create_source_target_files('./data/trainset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['    def tokenize(self, text, agressive_dash_splits=False, return_str=False):\\n',\n",
       "  '        \"\"\"\\n',\n",
       "  '        Python port of the Moses tokenizer.\\n',\n",
       "  '\\n',\n",
       "  '        >>> mtokenizer = MosesTokenizer()\\n',\n",
       "  \"        >>> text = u'Is 9.5 or 525,600 my favorite number?'\\n\",\n",
       "  '        >>> print (mtokenizer.tokenize(text, return_str=True))\\n',\n",
       "  '        Is 9.5 or 525,600 my favorite number ?\\n',\n",
       "  \"        >>> text = u'The https://github.com/jonsafari/tok-tok/blob/master/tok-tok.pl is a website with/and/or slashes and sort of weird : things'\\n\",\n",
       "  '        >>> print (mtokenizer.tokenize(text, return_str=True))\\n',\n",
       "  '        The https : / / github.com / jonsafari / tok-tok / blob / master / tok-tok.pl is a website with / and / or slashes and sort of weird : things\\n',\n",
       "  \"        >>> text = u'This, is a sentence with weird\\\\xbb symbols\\\\u2026 appearing everywhere\\\\xbf'\\n\",\n",
       "  \"        >>> expected = u'This , is a sentence with weird \\\\xbb symbols \\\\u2026 appearing everywhere \\\\xbf'\\n\",\n",
       "  '        >>> assert mtokenizer.tokenize(text, return_str=True) == expected\\n',\n",
       "  '\\n',\n",
       "  '        :param tokens: A single string, i.e. sentence text.\\n',\n",
       "  '        :type tokens: str\\n',\n",
       "  '        :param agressive_dash_splits: Option to trigger dash split rules .\\n',\n",
       "  '        :type agressive_dash_splits: bool\\n',\n",
       "  '        \"\"\"\\n',\n",
       "  '        # Converts input string into unicode.\\n',\n",
       "  '        text = text_type(text)\\n',\n",
       "  '\\n',\n",
       "  '        # De-duplicate spaces and clean ASCII junk\\n',\n",
       "  '        for regexp, substitution in [self.DEDUPLICATE_SPACE, self.ASCII_JUNK]:\\n',\n",
       "  '            text = re.sub(regexp, substitution, text)\\n',\n",
       "  '        # Strips heading and trailing spaces.\\n',\n",
       "  '        text = text.strip()\\n',\n",
       "  '        # Separate special characters outside of IsAlnum character set.\\n',\n",
       "  '        regexp, substitution = self.PAD_NOT_ISALNUM\\n',\n",
       "  '        text = re.sub(regexp, substitution, text)\\n',\n",
       "  '        # Aggressively splits dashes\\n',\n",
       "  '        if agressive_dash_splits:\\n',\n",
       "  '            regexp, substitution = self.AGGRESSIVE_HYPHEN_SPLIT\\n',\n",
       "  '            text = re.sub(regexp, substitution, text)\\n',\n",
       "  '        # Replaces multidots with \"DOTDOTMULTI\" literal strings.\\n',\n",
       "  '        text = self.replace_multidots(text)\\n',\n",
       "  '        # Separate out \",\" except if within numbers e.g. 5,300\\n',\n",
       "  '        for regexp, substitution in [self.COMMA_SEPARATE_1, self.COMMA_SEPARATE_2]:\\n',\n",
       "  '            text = re.sub(regexp, substitution, text)\\n',\n",
       "  '\\n',\n",
       "  '        # (Language-specific) apostrophe tokenization.\\n',\n",
       "  \"        if self.lang == 'en':\\n\",\n",
       "  '            for regexp, substitution in self.ENGLISH_SPECIFIC_APOSTROPHE:\\n',\n",
       "  '                 text = re.sub(regexp, substitution, text)\\n',\n",
       "  \"        elif self.lang in ['fr', 'it']:\\n\",\n",
       "  '            for regexp, substitution in self.FR_IT_SPECIFIC_APOSTROPHE:\\n',\n",
       "  '                text = re.sub(regexp, substitution, text)\\n',\n",
       "  '        else:\\n',\n",
       "  '            regexp, substitution = self.NON_SPECIFIC_APOSTROPHE\\n',\n",
       "  '            text = re.sub(regexp, substitution, text)\\n',\n",
       "  '\\n',\n",
       "  '        # Handles nonbreaking prefixes.\\n',\n",
       "  '        text = self.handles_nonbreaking_prefixes(text)\\n',\n",
       "  '        # Cleans up extraneous spaces.\\n',\n",
       "  '        regexp, substitution = self.DEDUPLICATE_SPACE\\n',\n",
       "  '        text = re.sub(regexp,substitution, text).strip()\\n',\n",
       "  '        # Restore multidots.\\n',\n",
       "  '        text = self.restore_multidots(text)\\n',\n",
       "  '        if not self.no_escape:\\n',\n",
       "  '            # Escape XML symbols.\\n',\n",
       "  '            text = self.escape_xml(text)\\n',\n",
       "  '\\n',\n",
       "  '        return text if return_str else text.split()\\n'],\n",
       " 320)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "inspect.getsourcelines(MosesTokenizer().tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
